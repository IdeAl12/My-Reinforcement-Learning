* Google机器学习速成课程
** 框架处理
*** 监督式学习
机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测。
*** 基本术语
**** 标签
在简单线性回归中，标签是我们要预测的事物，即 y 变量。
**** 特征
在简单线性回归中，特征是输入变量，即 x 变量。
**** 样本
样本实质数据的特定实例： *x* (粗体表示矢量)

将样本分为以下两类：

   *有标签样本：同时包含特征和标签，即：
  labeled examples: {features, label}: (x, y)

   *无标签样本 ：包含特征，但不包含标签，即：
  unlabeled examples: {features, ?}: (x, ?)
**** 模型
模型定义了特征与标签之间的关系。两个阶段：
***** 训练
表示创建或学习模型，向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。
***** 推断
表示将训练后的模型应用于无标签样本。
**** 回归于分类
回归模型课预测连续值。

分类模型课预测离散值。
** 降低损失(Reducing Loss):
*** 梯度下降法
第一个阶段是为w(权重)选择一个起始值。起点并不重要，因此很多算法就直接将w设为0或随机选择。

然后梯度下降算法会计算损失曲线在起点处的梯度。梯度是偏导数的矢量，梯度始终指向损失函数中增长最为迅猛的方向。梯度下降算法会沿着负梯度的方向走一步，以便尽快降低损失。（梯度下降法依赖于负梯度）

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加,一个梯度步长将我们移动到损失曲线上的下一个点,然后，梯度下降法会重复此过程，逐渐接近最低点。
*** 随机梯度下降法(SGD)
在梯度下降法中，批量指的是用于在单词迭代中计算梯度的样本总数。如果是超大批量，则单次迭代就可能要花很长时间进行计算。

包含随机抽样样本的大型数据及可能包含荣誉数据。实际上，批量大小越大，出现荣誉的可能性越高。一些荣誉可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

“随机”表示构成各个批量的一个样本都是随机选择的。。SGD每次迭代只使用一个样本。小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。
** 使用TensorFlow的起始步骤：工具包
*** tf.estimator API
使用tf.estimator会大大减少代码行数。

以下是在tf.estimator中实现的线性回归程序的格式。
#+BEGIN_SRC python
  import tensorflow as tf

  # Set up a linear classifier.
  classifier = tf.estimator.LinearClassifier()

  # Train the model on some example data.
  classifier.train(input_fn=train_input_fn, steps=2000)

  # Use it to predict.
  predictions = classifier.predict(input_fn=predict_input_fn)
#+END_SRC

*** 常用的超参数
**** steps
steps是指训练迭代的总次数。一步计算一批样本产生的损失，然后使用改制修改模型的权重一次。
**** batch size
batch size是指单步的样本数量（随机选择）。例如SGD的批量大小为1.

/total number of trained examples = batch size * steps/
** 泛化(generalization):过拟合的风险
过拟合模型在训练过程中产生的损失很低，但在预测新数据方面的表现却非常糟糕。过拟合是由于模型的福再度超出所需程度而造成的。机器学习的基本冲突时适当拟合我们的数据，但也要尽可能简单地拟合数据。

机器学习的目标是对真是概率分布（已隐藏）中抽取的新数据做出良好预测。遗憾的是模型无法查看整体情况；模型只能从训练数据集中取样。

奥卡姆剃刀定律在机器学习方面的运用：机器学习模型越简单，良好的实证结果就越有可能不仅仅基于样本的特性。

现今，已将奥卡姆剃刀定理正式应用于 *统计学习理论和计算学习理论* 领域。这些领域已经形成了 *泛化边界* ，即统计化描述模型根据以下因素泛化到新数据的能力：

 *模型的复杂程度；模型在处理训练数据方面的表现*

机器学习模型旨在根据以前未见过的新数据做出良好预测。一种方法是将数据集分成两个子集：训练集；测试集

一般来说，在测试集上表现是否良好是衡量能否在新数据上表现良好的有用指标，前提是：测试集足够大，不反复使用相同的测试集来作假。

*** 机器学习细则：
以下三项基本假设阐明了泛化：

1. 从分布中随机抽取独立同分布(i.i.d)的样本，即样本之间不会互相影响。

2. 分布是平稳的，即分布在数据集内不会发生变化。

3. 从 *同一分布* 的数据划分中抽取样本。

*** 总结：
如果某个模型尝试紧密拟合训练数据，却不能很好地泛化到新数据，就会发生过拟合。

如果不符合监督式机器学习的关键假设，那么将失去对新数据进行预测这项能力的重要理论保证。

** 训练集和测试集(Training and Test Sets):拆分数据
确保测试集满足两个条件：规模足够大，可产生具有统计意义的结果。能代表整个数据集，即挑选的测试集的特征应该与训练集的特征相同。

*请勿对测试数据进行训练* 。如果评估指标取得了意外的好结果，则可能表明不小心对测试集进行了训练。例如，高准确率可能表明测试数据泄漏刀客训练集。
** 验证(Validation):另一个划分
将数据集划分为两个子集是个不错的想法，但不是万能良方。通过将数据集划分为三个子集，可以大幅降低过拟合的发生几率。

使用验证集评估训练集的效果。然后，在模型通过验证机之后，使用测试集再次检查评估结果。

工作流程：

1. 选择在验证集上获得最佳效果的模型。

2. 使用测试集再次检查该模型。

该工作流程之所以更好，原因在于她暴露给测试集的信息更少。

*** 提示
不断使用测试集和验证集会使其逐渐失去效果。使用相同数据来决定超参数设置或其他模型改进的次数越多，对于这些结果能够真正泛化到未见过的新数据的信心就越低。请注意，验证集的失效速度通常比测试集缓慢。

如果可能的话，建议您收集更多数据来“刷新”测试集和验证集。重新开始是一种很好的重置方式。
** 表示(Representation)
*** 特征工程
传统编程的关注点是代码，而在机器学习项目中，关注点变成了表示，即开发者通过添加和改善特征来调整模型。
**** 将原始数据映射到特征
*特征工程* 指的是将原始数据转换为特征矢量，进行特征工程预计需要大量时间。机器学习模型通常期望样本表示为实数矢量。
**** 映射数值
机器学习模型根据浮点值进行训练，因此整数和浮点原始数据不需要特殊编码。
****  映射字符串值
模型无法通过字符串值学习规律，因此需要进行一些特征工程来将这些值转换为数字形式：

1. 首先，为要表示的所有特征的字符串值定义一个词汇表。

2. 然后，使用该词汇表创建一个独热编码(One-Hot),用于将指定字符串值表示为一个二元矢量。在该矢量(与指定的字符串值对应)中：只有一个元素设为1，其他所有元素均设为0。该矢量的长度等于词汇表中的元素数。

**** 映射分类（枚举）值
分类特征具有一组离散的可能只，通常将每个分类特征表示为单独的布尔值，采用这种编码还可以简化摸个之可能属于多个分类的情况。
*** 良好特征的特点
**** 避免很少使用的离散特征值
良好的特征值应该在数据集中出现大约5次以上。这样模型就可以学习该特征值与标签是如何关联的。也就是说，大量离散值相同的样本可让模型有机会了解不同设置中特征，从而判断何时可以对标签很好地作出预测。

相反，如果某个特征的值仅出现一次或者很少出现，则模型就无法根据该特征进行预测。
**** 最好具有清晰明确的含义
每个特征对于项目中的任何人来说都应该具有清晰明确的含义。在某些情况下，混乱的数据（而不是糟糕的工程选择）会导致含义不清晰的值。
**** 不要将“神奇”的值与实际数据混为一谈
良好的浮点特征不包含超出范围的异常断点或“神奇”的值。为解决神奇值的问题，需将该特征转换为两个特征：

+ 一个特征只存储质量评分，不含神奇值。

+ 一个特征存储布尔值，表示是否提供了质量评分。
**** 考虑上游不稳定性
特征的定义不应随时间发生变化。
*** 清理数据
**** 缩放特征值
缩放是指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。如果某个特征集只包含一个特征，则缩放可以提供的实际好处微乎其微或根本没有。不过，如果特征集包含多个特征，则缩放特征可以带来以下优势：

+ 帮助梯度下降法更快速地收敛。

+ 帮助避免“NaN 陷阱”。在这种陷阱中，模型中的一个数值变成 NaN（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 NaN。

+ 帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力。
***** 缩放方法
一种显而易见的方法是将 [最小值，最大值] 以线性方式映射到较小的范围，例如 [-1，+1]。

另一种热门的缩放策略是计算每个值的 Z 得分。Z 得分与距离均值的标准偏差数相关。即：

*scaledvalue = (value - mean)/stddev*

使用 Z 得分进行缩放意味着，大多数缩放后的值将介于 -3 和 +3 之间，而少量值将略高于或低于该范围。


**** 处理极端离群值
一种方法是对每个值取对数。对数缩放可稍稍缓解这种影响，但仍然存在离群值这个大尾巴。

另一种方式是限制最大值，将大于最大值的值都变成最大值。尽管存在小峰值，但是缩放后的特征集现在依然比原始数据有用。
**** 清查
在现实生活中，数据集中的很多样本是不可靠的，原因可能有： *遗漏值；重复样本；不良标签；不良特征值*

一旦检测到存在这些问题，通常需要将相应样本从数据集中移除，从而“修正”不良样本。要检测遗漏值或重复样本，可以编写一个简单的程序。检测不良特征值或标签可能会比较棘手。

除了检测各个不良样本之外，您还必须检测集合中的不良数据。直方图是一种用于可视化集合中数据的很好机制。此外，收集如下统计信息也会有所帮助：

+ 最大值和最小值

+ 均值和中间值

+ 标准偏差
**** 了解数据
遵循规则：

+ 记住预期的数据状态。

+ 确认数据是否满足这些预期（或者可以解释为何数据不满足预期）

+ 仔细检查训练数据是否与其他来源的数据一致。

*良好的机器学习依赖于良好的数据。*

** 特征组合(Feature Crosses):
*** 对非线性规律进行编码
对于非线性问题，可以创建一个特征组合。 *特征组合* 是指通过两个或多个输入特征相乘来对特征空间中的非线性规律进行编码的合成特征。
*** 组合独热矢量
在实践中，机器学习模型很少会组合连续特征，不过，机器学习模型经常组合独热特征矢量，将独热特征矢量的特征组合视为逻辑链接。

线性学习器可以很好地扩展到大量数据。对大规模数据集使用特征组合是学习高度复杂模型的一种有效策略。神经网络可提供另一种策略。
** 简单正则化(Regularization for Simplicity):
根据奥卡姆剃刀定律，或许可以通过降低复杂模型的复杂度来防止过拟合，这种原则称为 *正则化* 。也就是说并非只是以最小化损失（经验风险最小化）为目标：minimize(Loss(Data|Model)).而是以最小化损失和复杂度为目标：minimize(Loss(Data|Model) + complexity(Model)),这称为结构风险最小化。

现在训练优化算法是一个由两项内容组成的函数：一个是 *损失项* ，用于衡量模型与数据的拟合度，另一个是 *正则化项* ，用于衡量模型的复杂度。

两种衡量模型复杂度的常见方式：

+ 将模型复杂度作为模型中所有特征的权重的函数
。
+ 将模型复杂度作为具有非零权重的特征总数的函数。
*** L2正则化：
定义为所有特征权重的平方和。在这个公式中，接近于 0 的权重对模型复杂度几乎没有影响，而离群值权重则可能会产生巨大的影响。
*** lambda（正则化率）
用正则化项的值乘以名为 lambda（又称为正则化率）的标量。

*minimize(Loss(Data|Model) +\lambda complexity(Model))*

执行 L2 正则化对模型具有以下影响

+使权重值接近于 0（但并非正好为 0）

+使权重的平均值接近于 0，且呈正态（钟形曲线或高斯曲线）分布。

增加 lambda 值将增强正则化效果。

在选择 lambda 值时，目标是在简单化和训练数据拟合之间达到适当的平衡：

+如果 lambda 值过高，则模型会非常简单，但是将面临数据欠拟合的风险。模型将无法从训练数据中获得足够的信息来做出有用的预测。

+如果 lambda 值过低，则模型会比较复杂，并且将面临数据过拟合的风险。模型将因获得过多训练数据特点方面的信息而无法泛化到新数据。

理想的 lambda 值生成的模型可以很好地泛化到以前未见过的新数据。 遗憾的是，理想的 lambda 值取决于数据，因此需要手动或自动进行一些调整。
