* Google机器学习速成课程
** 框架处理
*** 监督式学习
机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测。
*** 基本术语
**** 标签
在简单线性回归中，标签是我们要预测的事物，即 y 变量。
**** 特征
在简单线性回归中，特征是输入变量，即 x 变量。
**** 样本
样本实质数据的特定实例： *x* (粗体表示矢量)

将样本分为以下两类：

   *有标签样本：同时包含特征和标签，即：
  labeled examples: {features, label}: (x, y)

   *无标签样本 ：包含特征，但不包含标签，即：
  unlabeled examples: {features, ?}: (x, ?)
**** 模型
模型定义了特征与标签之间的关系。两个阶段：
***** 训练
表示创建或学习模型，向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。
***** 推断
表示将训练后的模型应用于无标签样本。
**** 回归于分类
回归模型课预测连续值。

分类模型课预测离散值。
** 降低损失(Reducing Loss):
*** 梯度下降法
第一个阶段是为w(权重)选择一个起始值。起点并不重要，因此很多算法就直接将w设为0或随机选择。

然后梯度下降算法会计算损失曲线在起点处的梯度。梯度是偏导数的矢量，梯度始终指向损失函数中增长最为迅猛的方向。梯度下降算法会沿着负梯度的方向走一步，以便尽快降低损失。（梯度下降法依赖于负梯度）

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加,一个梯度步长将我们移动到损失曲线上的下一个点,然后，梯度下降法会重复此过程，逐渐接近最低点。
*** 随机梯度下降法(SGD)
在梯度下降法中，批量指的是用于在单词迭代中计算梯度的样本总数。如果是超大批量，则单次迭代就可能要花很长时间进行计算。

包含随机抽样样本的大型数据及可能包含荣誉数据。实际上，批量大小越大，出现荣誉的可能性越高。一些荣誉可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

“随机”表示构成各个批量的一个样本都是随机选择的。。SGD每次迭代只使用一个样本。小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。
** 使用TensorFlow的起始步骤：工具包
*** tf.estimator API
使用tf.estimator会大大减少代码行数。

以下是在tf.estimator中实现的线性回归程序的格式。
#+BEGIN_SRC python
  import tensorflow as tf

  # Set up a linear classifier.
  classifier = tf.estimator.LinearClassifier()

  # Train the model on some example data.
  classifier.train(input_fn=train_input_fn, steps=2000)

  # Use it to predict.
  predictions = classifier.predict(input_fn=predict_input_fn)
#+END_SRC

*** 常用的超参数
**** steps
steps是指训练迭代的总次数。一步计算一批样本产生的损失，然后使用改制修改模型的权重一次。
**** batch size
batch size是指单步的样本数量（随机选择）。例如SGD的批量大小为1.

/total number of trained examples = batch size * steps/
** 泛化(generalization):过拟合的风险
过拟合模型在训练过程中产生的损失很低，但在预测新数据方面的表现却非常糟糕。过拟合是由于模型的福再度超出所需程度而造成的。机器学习的基本冲突时适当拟合我们的数据，但也要尽可能简单地拟合数据。

机器学习的目标是对真是概率分布（已隐藏）中抽取的新数据做出良好预测。遗憾的是模型无法查看整体情况；模型只能从训练数据集中取样。

奥卡姆剃刀定律在机器学习方面的运用：机器学习模型越简单，良好的实证结果就越有可能不仅仅基于样本的特性。

现今，已将奥卡姆剃刀定理正式应用于 *统计学习理论和计算学习理论* 领域。这些领域已经形成了 *泛化边界* ，即统计化描述模型根据以下因素泛化到新数据的能力：

 *模型的复杂程度；模型在处理训练数据方面的表现*

机器学习模型旨在根据以前未见过的新数据做出良好预测。一种方法是将数据集分成两个子集：训练集；测试集

一般来说，在测试集上表现是否良好是衡量能否在新数据上表现良好的有用指标，前提是：测试集足够大，不反复使用相同的测试集来作假。

*** 机器学习细则：
以下三项基本假设阐明了泛化：

1. 从分布中随机抽取独立同分布(i.i.d)的样本，即样本之间不会互相影响。

2. 分布是平稳的，即分布在数据集内不会发生变化。

3. 从 *同一分布* 的数据划分中抽取样本。

*** 总结：
如果某个模型尝试紧密拟合训练数据，却不能很好地泛化到新数据，就会发生过拟合。

如果不符合监督式机器学习的关键假设，那么将失去对新数据进行预测这项能力的重要理论保证。

** 训练集和测试集(Training and Test Sets):拆分数据
确保测试集满足两个条件：规模足够大，可产生具有统计意义的结果。能代表整个数据集，即挑选的测试集的特征应该与训练集的特征相同。

*请勿对测试数据进行训练* 。如果评估指标取得了意外的好结果，则可能表明不小心对测试集进行了训练。例如，高准确率可能表明测试数据泄漏刀客训练集。
